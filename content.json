{"meta":{"title":"hbinnn's Blog","subtitle":null,"description":null,"author":"hbinnn","url":"http://hbinnn.github.io","root":"/"},"pages":[{"title":"分类","date":"2022-01-23T05:48:07.000Z","updated":"2022-01-23T06:58:35.944Z","comments":true,"path":"categories/index.html","permalink":"http://hbinnn.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-01-23T05:56:40.000Z","updated":"2022-01-23T06:58:44.236Z","comments":true,"path":"tags/index.html","permalink":"http://hbinnn.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"程序员的自我修养（一）","slug":"程序员的自我修养（一）","date":"2022-03-20T12:06:26.000Z","updated":"2022-03-20T12:10:46.786Z","comments":true,"path":"2022/03/20/程序员的自我修养（一）/","link":"","permalink":"http://hbinnn.github.io/2022/03/20/%E7%A8%8B%E5%BA%8F%E5%91%98%E7%9A%84%E8%87%AA%E6%88%91%E4%BF%AE%E5%85%BB%EF%BC%88%E4%B8%80%EF%BC%89/","excerpt":"","text":"构建过程可以分解成四个部分： 预编译； 编译； 汇编； 链接； 以 hello.c 为例来查看编译的整个流程。 1234567#include &lt;stdio.h&gt;int main()&#123; printf(&quot;hello world.\\n&quot;); return 0;&#125; 预编译通过 gcc -E 的方式得到预编译后的文件。 1gcc -E hello.c -o hello.i 预编译过程主要处理源代码中以 # 开始的预编译指令，主要处理规则： 删除所有的 #define，并展开所有的宏定义； 处理所有条件预编译指令，如 #if、#ifdef 等； 处理 #include 预编译指令，将被包含的文件插入到该预编译指令的位置； 所有的注释被替换成空格； 添加行号和文件号标识； 预处理后的文件内容以 # linenum filename flags 的形式存在，称为 linemarkers。 linenum 和 filename 表明以下内容来源于 filename 中的 linenum 行。以 hello.i 为例： 123456# 3 &quot;hello.c&quot;int main()&#123; printf(&quot;hello world.\\n&quot;); return 0;&#125; 保留所有的 #pragma； #pragma 用于给编译器提供额外的信息。 最常用的 #pragma once 告知编译器头文件只编译一次，与 #ifndef .. #define xxx #endif 的作用相同。 二者的区别在于 #pragma once 由编译器控制头文件只编译一次，但有的编译器并不支持 #pragma 方式，而 #ifndef 方式由使用者控制头文件编译，依赖于宏名称，容易出现不同的头文件使用了相同名称的宏导致编译问题。 编译编译过程就是将预编译完的文件进行一些列操作生成相应的汇编代码文件。 可以通过 gcc -S 的方式得到生成的汇编文件。 1gcc -S hello.i -o hello.s 编译过程可以分为 6 步：词法分析、语法分析、语义分析、源代码优化、代码生成和目标代码优化。 以一行代码为例： 12// hello.carray[index] = (index + 4) * (2 + 6); 词法分析源代码程序被输入到扫描器，扫描器将源代码的字符序列分割成一系列记号（Tokens），同时完成将标识符存放到符号表，将数字、字符串常量存放到文字表等工作。Tokens 一般可以分为：关键字、标识符、字面量、特殊符号。经过扫描得到了如下记号： array、[、index、]、=、(、index、+、4、)、*、(、2、+、6、)。 语法分析语法分析器对上述记号进行语法分析，从而生成以表达式为节点的语法树。 语义分析语义分析器在语义层面分析表达式是否具有实际意义。 静态语义指的是编译期就可以确定的语义，通常包含声明和类型的匹配、转换。 与静态语义对应的动态语义则是只有在运行期间才能确定的语义。 经过语义分析后，整个语法树的表达式都被标识了类型。 源代码优化源代码优化器会在源代码级别上进行一些优化，例如一些编译期可以确定值的表达式。 优化的过程往往将语法树转换成中间代码，中间代码与具体环境无关。常见的中间代码形式有：三地址码、P-代码。 以三地址码为例，上述语法树可以转换为如下中间代码： 123t2 = index + 2t2 = t2 + 8array[index] = t2 中间代码使得编译期可以被分为前段和后端。 前段负责产生与机器无关的中间代码，后端将中间代码转换为目标机器代码。 目标代码生成和优化代码生成器将中间代码转换成目标机器汇编代码，最后由目标代码优化器对生成的汇编代码进行优化。 汇编汇编过程主要处理将汇编代码转变为机器指令的过程。 可以通过 gcc -c 得到汇编处理后的文件。 1gcc -c hello.s -o hello.o 链接链接器主要处理模块间相互引用的部分，主要过程包括地址和空间分配、符号决议和重定位等。 最基本的静态链接如下所示，各模块编译的目标文件与库文件通过链接器形成最终的可执行文件。 参考文档 Top (The C Preprocessor) (gnu.org) 《程序员的自我修养》","categories":[{"name":"编译原理","slug":"编译原理","permalink":"http://hbinnn.github.io/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://hbinnn.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"}]},{"title":"SQL基本用法","slug":"SQL基本用法","date":"2022-02-13T08:33:04.000Z","updated":"2022-02-14T16:06:58.111Z","comments":true,"path":"2022/02/13/SQL基本用法/","link":"","permalink":"http://hbinnn.github.io/2022/02/13/SQL%E5%9F%BA%E6%9C%AC%E7%94%A8%E6%B3%95/","excerpt":"","text":"示例 SQL 语句中使用的表均可以在 《MySQL必知必会》 提供的网站中下载。 SELECT检索单个列1234SELECT cust_name FROM customers; 多个 SQL 语句必须以分号（；）分割。 SQL 语句不区分大小写。 检索多个列选择多个列时，列名之间用逗号（，）分割。 12345SELECT cust_name, cust_address FROM customers; 检索所有列在列名处使用通配符（*）替换。 1234SELECT * FROM customers; 检索不同的值当不希望检索结果出现同样的值时，可以使用关键字 DISTINCT 。DISTINCT 作用于多个列，当检索多个列时，只有多个列都完全相同时，检索结果才不会重复出现。 1234SELECT DISTINCT cust_name FROM customers; 限制检索结果的数量对 MySQL 而言，可以通过关键字 LIMIT 来限制检索显示的结果，只检索前几行。 12345SELECT cust_name FROM customers LIMIT 3; 如果希望从某一行开始检索几行，可以配合关键字 OFFSET 使用： 12345SELECT cust_name FROM customers LIMIT 3 OFFSET 1; ORDER BY排序数据123456SELECT cust_name FROM customers ORDER BY cust_zip; 通过 ORDER BY 子句指定列对检索出的数据进行排序。 ORDER BY 所指出的列并不是必须在 SELECT 指定，但是必须是表中已存在的列。 按多个列排序数据如果需要根据多个列排序，则使用 ORDER BY 指出多个列。 1234567SELECT cust_name FROM customers ORDER BY cust_zip, cust_country; 只有当 cust_zip 相同时才会根据 cust_country 进行排序。 按列位置排序ORDER BY 还支持按照检索列的相对位置进行排序。 1234567SELECT cust_name, cust_address FROM customers ORDER BY 2; ORDER BY 2 与 ORDER BY cust_address 的效果相同。 指定排序方向ORDER BY 默认是按照升序排序，如果需要降序排序，需要显式指定 DESC 关键字。 12345678SELECT cust_name, cust_address FROM customers ORDER BY cust_name, cust_address DESC; DESC 只应用于紧跟着的列名，如果需要对多个列进行降序排序，需要对每个列都指定 ORDER BY； 上述例子中，只会对 cust_address 进行降序排序，而不会对 cust_name 进行降序排序。 WHERE过滤数据1234567SELECT cust_name, cust_address FROM customers WHERE cust_email IS NULL; 数据根据 WHERE 子句中指定的条件进行过滤。WHERE 支持的操作符有： =、&lt;&gt;（不等于）、~=、&lt;、&lt;=、!&lt;（不小于）、&gt;、&gt;=、!&gt;（不大于）、BETWEEN .. AND …（在指定的两个值之间）、IS NULL（为 NULL 值）。 WHERE 与 OEDER BY 同时使用时，需要让 OEDER BY 位于 WHERE 之后。 组合 WHERE 子句使用 AND 或者 OR 关键字组合过滤条件。 12345678SELECT cust_name, cust_address FROM customers WHERE cust_name = &#x27;Fun4All&#x27; AND cust_state = &#x27;IN&#x27;; 指定条件范围通过关键字 IN 指定条件范围，范围内的条件都可以进行匹配。 1234567SELECT cust_name, cust_address FROM customers WHERE cust_name IN ( &#x27;Fun4All&#x27;, &#x27;Kids Place&#x27; ); 否定条件通过关键字 NOT 否定了 WHERE 子句中的过滤条件。 1234567SELECT cust_name, cust_address FROM customers WHERE NOT cust_email IS NULL; LIKE% 通配符% 表示匹配任意字符出现任意次数。 1234567SELECT prod_id, prod_name FROM products WHERE prod_name LIKE &#x27;% inch%&#x27;; _ 通配符_ 只匹配单个字符而不是任意个字符。 1234567SELECT prod_id, prod_name FROM products WHERE prod_name LIKE &#x27;_ inch%&#x27;; REGEXP基本字符匹配123456SELECT prod_name FROM products WHERE prod_name REGEXP &#x27;1000&#x27;; 上述例子也可以通过 LIKE 来实现。LIKE 与 REGEXP 的一个区别是，LIKE 需要被匹配的文件是整个列，而 REGEXP 只需要被匹配的文本在列中出现即可。 OR 匹配通过使用 | 来匹配多个正则表达式。 123456SELECT prod_name FROM products WHERE prod_name REGEXP &#x27;1000|2000&#x27;; [] 匹配[] 匹配其实就是另一种形式的 OR 匹配。 123456SELECT prod_name FROM products WHERE prod_name REGEXP &#x27;[123] ton&#x27;; 范围匹配可以通过 [] 定义一个范围来进行匹配。 123456SELECT prod_name FROM products WHERE prod_name REGEXP &#x27;[1-3] ton&#x27;; 同样可以否定 [] 支持的范围，只需要在 [] 内开始处加一个 ^。 123456SELECT prod_name FROM products WHERE prod_name REGEXP &#x27;[^1-3] ton&#x27;; 匹配特殊字符通过使用转义字符 \\\\ 来匹配特殊的字符。 123456SELECT vend_name FROM vendors WHERE vend_name REGEXP &#x27;\\\\.&#x27;; 匹配字符类可以使用预定义的字符集来匹配。 预定义字符集有： [:alnum:]：任意字母和数字； [:alpha:]：任意字母； [:digit:]：任意数字； [:xdigit:]：任意十六进制数字； [:blank:]：空格和制表符； [:sapce:]：任意空白字符； [:cntrl:]：ASCII 控制字符（0 - 31 以及 127）； [:print:]：任意可打印字符； [:graph:]：与 [:print:] 相同，但是不包括空格； [:lower:]：任意小写字母； [:upper:]：任意大写字母； [:punct:]：既不在 [:alnum:] 又不在 [:cntrl:] 中的任意字符； 匹配多个实例通过正则表达式重复元字符控制对正则表达式进行若干次匹配。 123456SELECT prod_name FROM products WHERE prod_name REGEXP &#x27;\\\\([0-9] sticks?\\\\)&#x27;; 重复元字符： *：0或多个匹配； +：1或多个匹配； ?：0或1个匹配； {n}：指定数目的匹配； {n, }：不少于指定数目的匹配； {n, m}：匹配数目的范围（m 不超过255）； 定位符通过定位符控制匹配的位置。 123456SELECT prod_priceFROM products WHERE prod_price REGEXP &#x27;^5&#x27;; 定位符： ^：文本的开始； $：文本的结尾； 拼接字段Concat()使用 Concat() 函数将多个列拼接成一个列。 1234SELECT CONCAT( prod_id, &#x27;(&#x27;, vend_id, &#x27;)&#x27; ) FROM products; 使用别名用 AS 将拼接出的字段赋予一个别名显示。 1234SELECT CONCAT( prod_id, &#x27;(&#x27;, vend_id, &#x27;)&#x27; ) AS title FROM products; 计算字段计算字段不实际存在于数据库表中，实在运行时 SELECT 语句内创建的。 对检索出的数据可以创建计算字段进行算术计算。 123456789SELECT prod_id, quantity, item_price, quantity * item_price AS price FROM orderitems WHERE order_num = 20005; 函数文本处理函数常见文本处理函数： Left(): 返回串左边的字符； Right(): 返回串右边的字符； 1234567SELECT vend_name, RIGHT ( vend_name, 5 ) AS vend_right_name FROM vendors ORDER BY vend_name; Length(): 返回串的长度； 1234567SELECT vend_name, LENGTH( vend_name ) AS vend_length FROM vendors ORDER BY vend_name; Locate(): 找出串的一个字串，返回字串第一次出现的位置，不存在返回0； 1234567SELECT vend_name, LOCATE( &#x27;ACM&#x27;, vend_name ) AS vend_substr FROM vendors ORDER BY vend_name; SubString(): 返回字串的字符； 1234567SELECT vend_name, substring( vend_name FROM 1 FOR 5 ) AS vend_substr FROM vendors ORDER BY vend_name; Lower(): 串转换为小写； Upper(): 串转换为大写； 1234567SELECT vend_name, UPPER( vend_name ) AS vend_upcaseFROM vendors ORDER BY vend_name; LTrim(): 去掉左边的空格; RTrim(): 去掉右边的空格; Trim(): 去掉两边的空格; 1234567SELECT vend_name, TRIM( vend_name ) AS vend_substr FROM vendors ORDER BY vend_name; 日期与时间处理函数MySQL 使用的日期格式为 YYYY-MM-DD。常用的日期与时间处理函数： AddDate(): 增加一个日期（天、周等）； AddTime(): 增加一个时间（时、分等）； 12345678SELECT order_date, ADDDATE( order_date, INTERVAL 1 YEAR ) AS next_year, ADDDATE( order_date, INTERVAL -1 MONTH ) AS last_month, ADDDATE( order_date, INTERVAL 1 DAY ) AS next_day, ADDTIME( order_date, &#x27;2:0:0&#x27; ) AS next_two_hour FROM orders; CurDate(): 返回当前日期； CurTime(): 返回当前时间； 12SELECT CONCAT( CURDATE(), &#x27; &#x27;, CURTIME()) AS time; Date(): 返回日期时间的日期部分； Time(): 返回日期时间的时间部分； Year(): 返回日期的年份部分； Month(): 返回日期的月份部分； Day(): 返回一个日期的天数部分； Hour(): 返回一个时间的小时部分； Minute(): 返回一个时间的分钟部分； Second(): 返回一个时间的秒部分； 1SELECT DAY(CURDATE()); 数值处理函数常见数值处理函数： Abs(): 返回绝对值； Rand(): 返回一个随机数； … 聚集函数常用聚集函数： AVG(): 返回某列的平均值； COUNT(): 返回某列的行数； MAX(): 返回某列的最大值； MIN(): 返回某列的最小值； SUM(): 返回某列值之和； 12345678SELECT AVG( prod_price ) AS avg_price, COUNT( prod_price ) AS price_num, MAX( prod_price ) AS max_price, min( prod_price ) AS min_price, SUM( prod_price ) AS sum_price FROM products; 在使用聚集函数时，如果希望只包含不同的值，可以使用关键字 DISTINCT。 12345SELECT COUNT( prod_price ) AS price_num, COUNT( DISTINCT prod_price ) AS price_DISTINCT_num,FROM products; GROUP BYGROUP BY 指出 检索出的数据是如何来分组的。 1234567SELECT vend_id, count(*) AS prod_num FROM products GROUP BY vend_id; SELECT 中指出的列都必须在 GROUP BY 出现，否则会出现非预期的结构。 GROUP BY 语句需要在 WHERE 之后，ORDER BY 之前。 过滤分组使用 HAVING 来过滤分组。WHERE 过滤的是行，不能用于分组。 123456789SELECT vend_id, count(*) AS prod_num FROM products GROUP BY vend_id HAVING count(*) &gt;= 3; 子查询子查询，即嵌套在其他查询中的查询，可以把一个 SELECT 语句的结果用于另一条 SELECT 语句的 WHERE 子句。 123456SELECT cust_id FROM orders WHERE order_num IN ( SELECT order_num FROM orderitems WHERE prod_id = &#x27;TNT2&#x27; ); 作为计算字段使用子查询12345678SELECT cust_name, cust_state, ( SELECT count(*) FROM orders WHERE orders.cust_id = customers.cust_id ) AS orders FROM customers ORDER BY cust_name; 这里计算字段的 WHERE 语句有点不一样，使用了完全限定列名，在列名可能出现二义性时就必须用这种方式。 联结联结用于组合来自多个表中的行。 （图片来源：SQL JOINS | 新手教程 (begtut.com)） 子查询中的例子也可以用如下方式查询： 12345678SELECT cust_id FROM orders INNER JOIN orderitems ON orders.order_num = orderitems.order_num AND prod_id = &#x27;TNT2&#x27;; 组合查询使用 UNION 组合多条 SQL 查询。 12345678910111213141516SELECT vend_id, prod_id, prod_price FROM products WHERE prod_price &lt; 5 UNIONSELECT vend_id, prod_id, prod_price FROM products WHERE vend_id IN ( 1001, 1002 ); UNION 查询时 多个 SQL 语句必须包含相同的列、表达式或聚集函数，SQL 语句见各个列并不需要按相同的顺序指出。 UNION 返回的数据自动去除了重复的行。如果需要，可以使用 UNION ALL。 12345678910111213141516SELECT vend_id, prod_id, prod_price FROM products WHERE prod_price &lt; 5 UNION ALLSELECT vend_id, prod_id, prod_price FROM products WHERE vend_id IN ( 1001, 1002 ); 如果需要对 UNION 返回的数据进行排序，只能有一条 ORDER BY 语句并放在最后一条 SQL 中，不存在用一种顺序排序一部分，另一种顺序排序另一部分。 123456789101112131415161718SELECT vend_id, prod_id, prod_price FROM products WHERE prod_price &lt; 5 UNION ALLSELECT vend_id, prod_id, prod_price FROM products WHERE vend_id IN ( 1001, 1002 ) ORDER BY prod_price; 全文本搜索MyISAM 引擎支持全文本搜索而 InnoDB 不支持。 启动全文本搜索支持： 12345678CREATE TABLE productnotes ( note_id int NOT NULL AUTO_INCREMENT, prod_id char(10) NOT NULL, note_date datetime NOT NULL, note_text text, PRIMARY KEY (note_id), FULLTEXT (note_text)) ENGINE=MyISAM 根据 FULLTEXT(note_text) 对它进行索引。 在索引之后，使用 Match() 和 Against() 执行全文本搜索，Match() 指定被索引的列，Against() 指定要使用的索引表达式。 123456SELECT note_text FROM productnotes WHERE MATCH ( note_text ) Against ( &#x27;rabbit&#x27; ); 全文本搜索返回的顺序按照文本匹配的良好程度排序，具有较高等级的行先返回。 查看排序计算的等级值： 12345SELECT note_text, MATCH ( note_text ) Against ( &#x27;rabbit&#x27; ) as rank_FROM productnotes; 查询扩展查询扩展不仅返回所有匹配的行，还将返回与匹配行相关的行。 123456SELECT note_text FROM productnotes WHERE MATCH ( note_text ) against ( &#x27;anvils&#x27; WITH QUERY EXPANSION ); 布尔文本查询布尔方式可以提供关于内容的细节： 要匹配的词； 要排斥的词； 排列提示； 表达式分组； 123456SELECT note_text FROM productnotes WHERE MATCH ( note_text ) against ( &#x27;heavy -rope*&#x27; IN BOOLEAN MODE ); 支持的全文本布尔操作符有： +：词必须包含； -：词必须排除； &gt;：包含，增加等级值； &lt;：包含，减少等级值； ()：把词组成表达式； ~：取消一个词的排序值； *：词尾的通配符； “”：定义一个短语； INSERT插入完整的行 123INSERT INTO customersVALUES ( NULL, &#x27;Pen&#x27;, &#x27;100 Street&#x27;, &#x27;HANGZHOU&#x27;, &#x27;AA&#x27;, &#x27;1111&#x27;, &#x27;CN&#x27;, &#x27;NULL&#x27;, &#x27;NULL&#x27; ); INSERT 一般不会产生输入。 这种插入方式需要依赖表中列的定义顺序，可以在 INSERT 语句中指出要插入的列来避免这种情况。 1234567INSERT INTO customers ( cust_id, cust_name, cust_address, cust_city, cust_state, cust_zip, cust_country, cust_contact, cust_email )VALUES ( NULL, &#x27;Pen&#x27;, &#x27;100 Street&#x27;, &#x27;HANGZHOU&#x27;, &#x27;AA&#x27;, &#x27;1111&#x27;, &#x27;CN&#x27;, &#x27;NULL&#x27;, &#x27;NULL&#x27; );SELECT * FROM customers; 插入多个行，只要在 VALUES 之间用逗号分隔开即可 插入检索出的数据INSERT 还可以与 SELECT 配合使用将 SELECT 检索出的结果插入到表中。 1234567INSERT INTO newproductnotes ( note_id, prod_id, note_date, note_text ) SELECTnote_id,prod_id,note_date,note_text FROM productnotes; 实际上，SELECT 与 INSERT 并不要求列名匹配。MySQL 在填充时使用的是列的位置，也即 SELECT 返回的第一列用于填充 INSERT 字段中的第一列。 UPDATE使用 WHERE 筛选出需要更新的行，如果不筛选，则更新所有行。 1234UPDATE newproductnotes SET note_date = ADDDATE( note_date, INTERVAL 1 DAY ) WHERE note_id &lt; 110; 更新多个列时只要用 set 设置多个列即可。 在使用 UPDATE 更新时，如果出现错误，则整个 UPDATE 操作被取消。如果希望发生错误也继续更新，可以使用 IGNORE 关键字。UPDATE IGNORE… 。 DELETE与 UPDATE 相同，同样使用 WHERE 筛选所需要的行进行删除。 12345DELETE FROM newproductnotes WHERE note_id &lt; 110; CREATE使用 CREATE TABLE 创建表。 12345678CREATE TABLE newproductnotes2 ( note_id int NOT NULL AUTO_INCREMENT, prod_id char(10) NOT NULL, note_date datetime NOT NULL DEFAULT &#x27;1970-01-01 00:00:00&#x27;, note_text text, PRIMARY KEY (note_id), FULLTEXT (note_text)) ENGINE=MyISAM CREATE TABLE 之后跟表名。在创建新表时，指定的表名应当不存在，否则会出错。 NOT NULL 指出该列的值不能为 NULL，也即插入数据时该列不能为空。允许为 NULL 值的列在插入数据时可以不给出值。如果不显式指定，默认时 NULL。 PRIMARY KEY 指出表的主键。主键值必须唯一。如果使用多个主键，则组合值必须唯一。 AUTO_INCREMENT 指出在每增加一行时该列的值自动增加，如此在插入数据时可以不指定该列的值。当然也可以在插入时指定值，只要它在表中时唯一的即可，后续增量将从该手工插入的值开始。每个表只能有一个 AUTO_INCREMENT 列，并且它必须被索引。 经实验，AUTO_INCREMENT 的递增值只有在手工插入值比其大时才会变更，插入值比当前递增值小时不会变。假设 下一个递增值是 110，此时手动指定108插入，下一次不指定让其自动递增是111，不会变成109。 创建表时可以使用关键字 DEFAULT 指定列中的默认值，在插入时如果未指定值则使用默认值。 ENGINE 指出表使用的内部引擎。 更新表使用 ALTER TABLE 更新表结构。 添加列： 1ALTER TABLE newproductnotes2 ADD note_type CHAR ( 20 ); 删除列： 1ALTER TABLE newproductnotes2 DROP note_type; ALTER TABLE 也可以用来定义外键。 123ALTER TABLE newproductnotes2 ADD CONSTRAINT fk_note_id FOREIGN KEY ( note_id ) REFERENCES newproductnotes ( note_id ); 删除表1DROP TABLE newproductnotes2; 重命名表1RENAME TABLE newproductnotes TO np2; 建立索引在 CREATE 语句内 使用 [KEY|INDEX] 索引名 (索引列) 的方式创建索引。 123456789CREATE TABLE newproductnotes2 ( note_id INT NOT NULL AUTO_INCREMENT, prod_id CHAR ( 10 ) NOT NULL, note_date datetime NOT NULL DEFAULT &#x27;1970-01-01 00:00:00&#x27;, note_text text, PRIMARY KEY ( note_id ), FULLTEXT ( note_text ), INDEX idx1 ( note_id, note_date ) ) ENGINE = MyISAM 修改、删除索引可以在修改表结构的时候添加索引： 1ALTER TABLE 表名 ADD [KEY|INDEX] 索引名 (索引列); 同样也可以删除索引： 1ALTER TABLE 表名 DROP [KEY|INDEX] 索引名; 查看索引12SHOW INDEX FROM newproductnotes2; 视图视图是虚拟的表，只包含使用时动态检索数据的查询。简单来说，视图可以将一个复杂的 SQL 语句包装起来，简化复杂的操作。 1234567891011SELECT cust_name, cust_contact FROM customers, orders, orderitems WHERE customers.cust_id = orders.cust_id AND orderitems.order_num = orders.order_num AND prod_id = &#x27;TNT2&#x27;; 可以通过创建视图简化上述操作。 CREATE VIEW viewname; 创建一个新的视图。 SHOW CREATE VIEW viewname; 查看创建的视图。 DROP VIEW viewname; 删除视图。 更新视图可以先 DROP 再 CREATE，也可以直接使用 CREATE OR REPLACE VIEW 来更新。 1234567891011CREATE VIEW productcustomers AS SELECTcust_name,cust_contact, prod_id FROM customers, orders, orderitems WHERE customers.cust_id = orders.cust_id AND orderitems.order_num = orders.order_num; 1234567SELECT cust_name, cust_contact FROM productcustomers WHERE prod_id = &#x27;TNT2&#x27;; 更新视图由于视图只是一个虚拟的表，更新视图实际上是对视图中所使用的表进行更新。但不是所有的视图都可以更新。包含以下操作的视图不能进行更新： GROUP BY 、 HAVING； 联结； 子查询； 并； 聚集函数； DISTINCT； 计算字段； 存储过程存储过程，即一条或多条 SQL 语句的集合。 创建存储过程1234567891011121314151617CREATE PROCEDURE productpricing ( OUT pl DECIMAL ( 8, 2 ), OUT ph DECIMAL ( 8, 2 ), OUT pa DECIMAL ( 8, 2 )) BEGIN SELECT MIN( prod_price ) INTO pl FROM products; SELECT MAX( prod_price ) INTO ph FROM products; SELECT AVG( prod_price ) INTO pa FROM products;END; 存储过程可以接受参数。关键字 OUT 表明相应参数从存储过程中传出，IN 表明相应参数从外传递给存储过程，INOUT则即传入又传出。 INTO 关键字表明将结果存储给相应的变量。 可以使用 DECLARE 在存储过程内部定义局部变量。例如 1DECLARE total DECIMAL(8, 2); 存储过程 END 结尾也是使用；作为分隔符。这样有可能导致存储过程内部的；不会被当作存储过程的一部分。因此可以通过临时更改分隔符的方式解决。 123456789DELIMITER //CREATE PROCEDURE productpricing2 () BEGIN SELECT AVG( prod_price ) FROM products; END // DELIMITER; 执行存储过程12345CALL productpricing ( @pricelow, @pricehigh, @priceaverage );SELECT @pricelow, @pricehigh, @priceaverage; 所有变量都要以 @ 开始。 删除存储过程1DROP PROCEDURE productpricing; 查看存储过程1SHOW CREATE PROCEDURE productpricing; 获得详细的存储过程列表，可以使用 SHOW PROCEDURE STATUS; 游标游标主要用于在检索出的数据中前进或者后退一行。MySQL 中游标只能用于存储过程。 在使用游标前必须先定义，声明后必须打开才能使用，使用后必须关闭游标。 创建游标1234567CREATE PROCEDURE processorders () BEGIN DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders;END; 打开和关闭游标12OPEN ordernumbers;CLOSE ordernumbers; 使用游标123456789101112131415161718192021222324252627282930CREATE PROCEDURE processorders () BEGIN DECLARE done BOOLEAN DEFAULT 0; DECLARE o INT; DECLARE t DECIMAL ( 8, 2 ); DECLARE ordernumbers CURSOR FOR SELECT order_num FROM orders; DECLARE CONTINUE HANDLER FOR SQLSTATE &#x27;02000&#x27; SET done = 1; CREATE TABLE IF NOT EXISTS ordertotals ( order_num INT, total DECIMAL ( 8, 2 )); OPEN ordernumbers; REPEAT FETCH ordernumbers INTO o; CALL ordertotal ( o, 1, t ); INSERT INTO ordertotals ( order_num, total ) VALUES ( o, t ); UNTIL done END REPEAT; CLOSE ordernumbers;END; 触发器在某个表发生更改时自动处理，这就是触发器。 触发器只会响应 DELETE、INSERT、UPDATE 从而自动执行一条或多条 SQL 语句。 每个表每个事件每次只允许一个触发器，因此每个表最多支持 6 个触发器。 创建触发器需要给出 4 个必要的信息： 触发器名，每个表唯一； 触发器关联的表； 触发器应该响应的动作； 触发器何时执行； 1234CREATE TRIGGER newproduct AFTER INSERT ON products FOR EACH ROWSELECT &#x27;Product added&#x27; INTO @result; 删除触发器1DROP TRIGGER newproduct; 使用触发器INSERT 触发器 INSERT 触发器内可以引用一个名为 NEW 的虚拟表，访问被插入的行； BEFORE INSERT 触发器内，NEW 中的值也可以被更新； 对于 AUTO_INCREMENT 列，NEW 在 INSERT 执行前包含0，执行之后包含自动生成的值； 1234CREATE TRIGGER newproduct AFTER INSERT ON products FOR EACH ROWSELECT NEW.prod_id INTO @result; DELETE 触发器 DELETE 触发器内可以引用一个名为 OLD 的虚拟表，访问被删除的行； OLD 中的值都是只读的，不能更新； UPDATE 触发器 UPDATE 触发器内可以引用一个 OLD 的虚拟表访问更新前的值，NEW 的虚拟表访问更新后的值； OLD 中的值都是只读的，不能更新； NEW 中的值可以被更新； 事务事务处理机制用来管理成批执行的 SQL 操作，保证数据库中不存在不完整操作的结果。一批 SQL 操作要么整体执行完毕，要么都不执行。 事务的开始1START TRANSACTION; ROLLBACK123456SELECT * FROM products;START TRANSACTION;DELETE FROM products;SELECT * FROM products;ROLLBACK;SELECT * FROM products; ROLLBACK 用来回退 SQL 操作，只能在事务处理内使用。 COMMIT在事务内，提交不会隐含执行，必须使用 COMMIT 显式提交。 保留点类似于存档，用于回退时需要回到哪个执行阶段。 SAVEPOINT point；创建保留点。 ROLLBACK TO point；回退到指定的保留点。 更改默认的提交行为默认的 SQL 行为是自动提交的，可以通过修改 autocommit 标志修改这种行为。 set autocommit = 0；","categories":[{"name":"数据库","slug":"数据库","permalink":"http://hbinnn.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"}],"tags":[{"name":"MySQL","slug":"MySQL","permalink":"http://hbinnn.github.io/tags/MySQL/"}]},{"title":"博客大杂烩","slug":"博客大杂烩","date":"2022-02-03T11:07:28.000Z","updated":"2022-02-12T12:41:23.025Z","comments":true,"path":"2022/02/03/博客大杂烩/","link":"","permalink":"http://hbinnn.github.io/2022/02/03/%E5%8D%9A%E5%AE%A2%E5%A4%A7%E6%9D%82%E7%83%A9/","excerpt":"","text":"操作系统CPU 调度原理：https://mp.weixin.qq.com/s/befER-WAkLnpIxSvaae6zw 持续更新中…","categories":[],"tags":[{"name":"大杂烩","slug":"大杂烩","permalink":"http://hbinnn.github.io/tags/%E5%A4%A7%E6%9D%82%E7%83%A9/"}]},{"title":"Windows与Linux下的端口转发","slug":"Windows与Linux下的端口转发","date":"2022-01-23T15:45:54.000Z","updated":"2022-01-23T15:50:49.073Z","comments":true,"path":"2022/01/23/Windows与Linux下的端口转发/","link":"","permalink":"http://hbinnn.github.io/2022/01/23/Windows%E4%B8%8ELinux%E4%B8%8B%E7%9A%84%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/","excerpt":"","text":"主要涉及的命令有 netsh 与 iptables。 netsh曾经遇到过这样一个场景： PC 可以与 linux1、linux2 互通，但是 linux1 与 linux2 之间不通，linux1 直接访问 linux2 无法成功，也无法通过添加路由的方式访问。此时可以利用 PC 配置端口转发达到访问的目的。 利用 Windows 下的 netsh 命令来配置网络端口转发的功能。 netsh interface portproxy add v4tov4 listenaddress=address listenport=port connectaddress=destaddress connectport=destport protocol=tcp listenaddress：监听地址； listenport：监听端口； connectaddress：端口转发需要连接的目的地址； connectport：端口转发需要连接的目的端口； 配置端口转发之后，连接到 listenaddress:listenport 的 tcp 流量就会被转发到 connectaddress:connectport 中。 查看系统中所有转发规则是否生效：netsh interface portproxy show all； 查看端口转发的设置：netsh interface portproxy dump； 删掉一个特定的端口转发规则：netsh interface portproxy delete v4tov4； 清空当前所有的配置规则：netsh interface portproxy reset； iptablesiptables 是 Linux 下的一个包过滤防火墙管理工具，通过设置过滤规则实现对数据包进出设备以及转发的控制。 iptables 维护着 4 个表和 5 个链，所有的防火墙策略规则都会被写入这些表和链中。 4 个表主要指的是： filter 表：控制数据包是否允许放行，可以控制的链路有 INPUT、FORWARD 和 OUTPUT。 nat 表：控制数据包中地址转换，可以控制的链路有 PREROUTING、INPUT、OUTPUT 和 POSTROUTING。属于同一个流的包只会经过这个表一次。 mangle：修改数据包中的原数据，可以控制的链路有 PREROUTING、INPUT、OUTPUT、FORWARD 和 POSTROUTING。主要用于修改数据包的TOS(Type Of Service，服务类型)、TTL(Time To Live，生存周期)指以及为数据包设置Mark标记，以实现Qos(Quality Of Service，服务质量)调整以及策略路由等应用。 raw：控制数据包是否被状态跟踪机制处理，可以控制的链路有 PREROUTING、OUTPUT。 5个链主要指的是： INPUT：进来的数据包应用此规则链中的策略 OUTPUT：外出的数据包应用此规则链中的策略 FORWARD：转发数据包时应用此规则链中的策略 PREROUTING：对进来的数据包作路由选择前应用此链中的规则 POSTROUTING：对外出的数据包作路由选择后应用此链中的规则 数据报文必须按顺序匹配这些链上的规则。每条链上的各个表的匹配顺序为：raw→mangle→nat→filter。 数据包通过防火墙的流程可以总结为： iptables的命令主要格式如下： iptables [-t 表名] command 匹配规则 常用命令选项： -A 添加防火墙规则 -D 删除防火墙规则 -I 插入防火墙规则 -F 清空防火墙规则 -L 列出添加防火墙规则 -R 替换防火墙规则 -Z 清空防火墙数据表统计信息 -P 设置链默认规则 常用匹配参数： [!]-p 匹配协议，! 表示取反 [!]-s 匹配源地址 [!]-d 匹配目标地址 [!]-i 匹配入站网卡接口 [!]-o 匹配出站网卡接口 [!]–sport 匹配源端口 [!]–dport 匹配目标端口 [!]–src-range 匹配源地址范围 [!]–dst-range 匹配目标地址范围 [!]–limit 四配数据表速率 [!]–mac-source 匹配源MAC地址 [!]–sports 匹配源端口 [!]–dports 匹配目标端口 [!]–stste 匹配状态（INVALID、ESTABLISHED、NEW、RELATED) [!]–string 匹配应用层字串 匹配规则触发动作： ACCEPT 允许数据包通过 DROP 丢弃数据包 REJECT 拒绝数据包通过 LOG 将数据包信息记录 syslog 曰志 DNAT 目标地址转换 SNAT 源地址转换 MASQUERADE 地址欺骗 REDIRECT 重定向 参考资料 Windows端口转发(Port Forwarding in Windows) - 简书 (jianshu.com) iptables详解 | Bruce’s Blog (xiebruce.top) iptables详解及一些常用规则 - 简书 (jianshu.com) Linux iptables命令详解【图文】_wx607823dfcf6a9_51CTO博客","categories":[{"name":"网络","slug":"网络","permalink":"http://hbinnn.github.io/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://hbinnn.github.io/tags/Linux/"},{"name":"端口转发","slug":"端口转发","permalink":"http://hbinnn.github.io/tags/%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/"}]},{"title":"文件系统","slug":"文件系统","date":"2022-01-23T07:21:18.000Z","updated":"2022-01-23T15:46:55.144Z","comments":true,"path":"2022/01/23/文件系统/","link":"","permalink":"http://hbinnn.github.io/2022/01/23/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"文章主要摘自博客以及现代操作系统第四章，顺序可能略有调整。 博客原文链接:https://mp.weixin.qq.com/s/qJdoXTv_XS_4ts9YuzMNIw 作者:小林coding 为什么要有文件系统 进程的存储容量受到虚拟地址空间大小的限制； 有些信息需要持久保留； 有些信息需要在多个进程之间共享； 文件系统是什么文件系统是操作系统中负责管理持久数据的子系统。 文件系统的基本数据单位是文件，它的目的是对磁盘上的文件进行组织管理，组织的方式不同，就会形成不同的文件系统。 文件系统类型Linux 支持的文件系统也不少，根据存储位置的不同，可以把文件系统分为三类： 磁盘的文件系统，它是直接把数据存储在磁盘中，比如 Ext 2/3/4、XFS 等都是这类文件系统。 内存的文件系统，这类文件系统的数据不是存储在硬盘的，而是占用内存空间，我们经常用到的 /proc 和 /sys 文件系统都属于这一类，读写这类文件，实际上是读写内核中相关的数据数据。 网络的文件系统，用来访问其他计算机主机数据的文件系统，比如 NFS、SMB 等等 文件文件是进程创建的信息逻辑单元。 文件命名有些文件系统不区分文件名大小写字母，如 MS-DOS，有些则区分，如 UNIX。 有些文件系统文件扩展名只是一种约定，并不强迫使用它，如 UNIX，有些则对扩展名赋予了含义，用户可以在操作系统中注册扩展名，并规定哪个程序“拥有”该扩展名，如 Windows。 文件结构对常见的 UNIX 和 Windows 而言，操作系统不关系文件的内容，操作系统见到的就是字节，文件内容的任何含义只在用户程序中解释。 文件类型 普通文件和类型； 字符特殊文件，和输入/输出有关，用于串行 I/O 类设备； 块特殊文件，用于磁盘类设备； 文件访问 顺序访问，从头开始读取文件的全部字节，不能跳过某一些内容，也无法不按顺序读取； 随机访问文件，以任意次序读取文件的字节； 文件属性 文件使用 我们打开了一个文件后，操作系统会跟踪进程打开的所有文件，所谓的跟踪，就是操作系统为每个进程维护一个打开文件表，文件表里的每一项代表「文件描述符」。 操作系统在打开文件表中维护着打开文件的状态和信息： 文件指针：系统跟踪上次读写位置作为当前文件位置指针，这种指针对打开文件的某个进程来说是唯一的； 文件打开计数器：文件关闭时，操作系统必须重用其打开文件表条目，否则表内空间不够用。因为多个进程可能打开同一个文件，所以系统在删除打开文件条目之前，必须等待最后一个进程关闭文件，该计数器跟踪打开和关闭的数量，当该计数为 0 时，系统关闭文件，删除该条目； 文件磁盘位置：绝大多数文件操作都要求系统修改文件数据，该信息保存在内存中，以免每个操作都从磁盘中读取； 访问权限：每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开文件表中，以便操作系统能允许或拒绝之后的 I/O 请求； 用户和操作系统对文件的读写操作是有差异的，用户习惯以字节的方式读写文件，而操作系统则是以数据块来读写文件，那屏蔽掉这种差异的工作就是文件系统了。 我们来分别看一下，读文件和写文件的过程： 当用户进程从文件读取 1 个字节大小的数据时，文件系统则需要获取字节所在的数据块，再返回数据块对应的用户进程所需的数据部分。 当用户进程把 1 个字节大小的数据写进文件时，文件系统则找到需要写入数据的数据块的位置，然后修改数据块中对应的部分，最后再把数据块写回磁盘。 所以说，文件系统的基本操作单位是数据块。 文件系统实现文件系统布局文件系统存放在磁盘上，磁盘通常被划分为一个或多个分区。 磁盘的 0 号扇区成为主引导记录（Master Boot Record，MBR），MBR 的结尾是分区表，记录着每个分区的起始和结束地址。 分区表中的一个分区被标记为活动分区，它的第一个块被称为引导块，引导块中的程序将装载该分区中的操作系统。 从引导块开始，不同的文件系统，磁盘分区的布局也不一样的。 文件系统如何读写磁盘磁盘读写的最小单位是扇区，扇区的大小只有 512B 大小。 对文件系统而言，如果每次读写都以扇区为单位，那读写的效率会非常低。 所以，文件系统把多个扇区组成了一个逻辑块，每次读写的最小单位就是逻辑块（数据块），Linux 中的逻辑块大小为 4KB，也就是一次性读写 8 个扇区，这将大大提高了磁盘的读写的效率。 索引节点是存储在硬盘上的数据，那么为了加速文件的访问，通常会把索引节点加载到内存中。 超级块，用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等。 索引节点区，用来存储索引节点； 数据块区，用来存储文件或目录数据； 我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使用的时候，才将其加载进内存，它们加载进内存的时机是不同的： 超级块：当文件系统挂载时进入内存； 索引节点区：当文件被访问时进入内存； Linux 文件系统会为每个文件分配两个数据结构：索引节点（index node）和目录项（directory entry），它们主要用来记录文件的元信息和目录层次结构。 索引节点，也就是 inode（上图中的i节点）用来记录文件的元信息，比如 inode 编号、文件大小、访问权限、创建时间、修改时间、数据在磁盘的位置等等。索引节点是文件的唯一标识，它们之间一一对应，也同样都会被存储在硬盘中，所以索引节点同样占用磁盘空间。 目录项，也就是 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存。 目录项和索引节点的关系是多对一。 目录也是文件，也是用索引节点唯一标识，和普通文件不同的是，普通文件在磁盘里面保存的是文件数据，而目录文件在磁盘里面保存子目录或文件。 文件存储文件的数据是要存储在硬盘上面的，数据在磁盘上的存放方式，就像程序在内存中存放的方式那样，有以下两种： 连续空间存放方式 非连续空间存放方式 其中，非连续空间存放方式又可以分为「链表方式」和「索引方式」。 连续空间存放方式连续空间存放方式顾名思义，文件存放在磁盘「连续的」物理空间中。 这种分配方式有两大优势： 文件的数据都是紧密相连，读写效率很高，因为一次磁盘寻道就可以读出整个文件； 实现简单，只需要记录每个文件的起始磁盘块地址和文件的磁盘块数，就可以确定一个文件； 所以，文件头里需要指定「起始块的位置」和「长度」，有了这两个信息就可以很好的表示文件存放方式是一块连续的磁盘空间。 注意，此处说的文件头，就类似于 Linux 的 inode。 连续空间存放的方式虽然读写效率高，但是有「磁盘空间碎片」和「文件长度不易扩展」的缺陷。 必须先知道一个文件的大小，这样文件系统才会根据文件的大小在磁盘上找到一块连续的空间分配给文件。 对一些只读文件系统，连续空间存放的方案反而比较高效。CD-ROM、DVD 等。 非连续空间存放方式非连续空间存放方式分为「链表方式」和「索引方式」。 链表方式链表的方式存放是离散的，不用连续的，于是就可以消除磁盘碎片，可大大提高磁盘空间的利用率，同时文件的长度可以动态扩展。根据实现的方式的不同，链表可分为「隐式链表」和「显式链接」两种形式。 隐式链表文件要以「隐式链表」的方式存放的话，实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置，这样一个数据块连着一个数据块，从链头开是就可以顺着指针找到所有的数据块，所以存放的方式可以是不连续的。 隐式链表的存放方式的缺点在于无法直接访问数据块，只能通过指针顺序访问文件，以及数据块指针消耗了一定的存储空间。 另一方面，由于指针占用了一些字节，每个磁盘块中存储数据的字节数不再是2的整数次幂，对于以长度为2的整数次幂来读写磁盘块的程序而言，降低了运行效率。 显式链接如果取出每个磁盘块的指针，把它放在内存的一个表中，就可以解决上述隐式链表的两个不足。那么，这种实现方式是「显式链接」，它指把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中，该表在整个磁盘仅设置一张，每个表项中存放链接指针，指向下一个数据块号。 对于显式链接的工作方式，我们举个例子，文件 A 依次使用了磁盘块 4、7、2、10 和 12 ，文件 B 依次使用了磁盘块 6、3、11 和 14 。利用下图中的表，可以从第 4 块开始，顺着链走到最后，找到文件 A 的全部磁盘块。同样，从第 6 块开始，顺着链走到最后，也能够找出文件 B 的全部磁盘块。最后，这两个链都以一个不属于有效磁盘编号的特殊标记（如 -1 ）结束。内存中的这样一个表格称为文件分配表（File Allocation Table，FAT）。 由于查找记录的过程是在内存中进行的，因而不仅显著地提高了检索速度，而且大大减少了访问磁盘的次数。但也正是整个表都存放在内存中的关系，它的主要的缺点是不适用于大磁盘。 比如，对于 200GB 的磁盘和 1KB 大小的块，这张表需要有 2 亿项，每一项对应于这 2 亿个磁盘块中的一个块，每项如果需要 4 个字节，那这张表要占用 800MB 内存，很显然 FAT 方案对于大磁盘而言不太合适。 索引方式链表的方式解决了连续分配的磁盘碎片和文件动态扩展的问题，但是不能有效支持直接访问（FAT除外），索引的方式可以解决这个问题。 索引的实现是为每个文件创建一个「索引数据块」，里面存放的是指向文件数据块的指针列表，说白了就像书的目录一样，要找哪个章节的内容，看目录查就可以。 另外，文件头需要包含指向「索引数据块」的指针，这样就可以通过文件头知道索引数据块的位置，再通过索引数据块里的索引信息找到对应的数据块。 创建文件时，索引块的所有指针都设为空。当首次写入第 i 块时，先从空闲空间中取得一个块，再将其地址写到索引块的第 i 个条目。 索引的方式优点在于： 文件的创建、增大、缩小很方便； 不会有碎片的问题； 支持顺序读写和随机读写； 由于索引数据也是存放在磁盘块的，如果文件很小，明明只需一块就可以存放的下，但还是需要额外分配一块来存放索引数据，所以缺陷之一就是存储索引带来的开销。 如果文件很大，大到一个索引数据块放不下索引信息，这时又要如何处理大文件的存放呢？我们可以通过组合的方式，来处理大文件的存。 先来看看链表 + 索引的组合，这种组合称为「链式索引块」，它的实现方式是在索引数据块留出一个存放下一个索引数据块的指针，于是当一个索引数据块的索引信息用完了，就可以通过指针的方式，找到下一个索引数据块的信息。 这种方式也会出现前面提到的链表方式的问题。 还有另外一种组合方式是索引 + 索引的方式，这种组合称为「多级索引块」，实现方式是通过一个索引块来存放多个索引数据块，一层套一层索引。 Unix 文件存储方式我们先把前面提到的文件存储方式，做个比较： 那早期 Unix 文件系统是组合了前面的文件存放方式的优点，如下图： 它是根据文件的大小，存放的方式会有所变化： 如果存放文件所需的数据块小于 10 块，则采用直接查找的方式； 如果存放文件所需的数据块超过 10 块，则采用一级间接索引方式； 如果前面两种方式都不够存放大文件，则采用二级间接索引方式； 如果二级间接索引也不够存放大文件，这采用三级间接索引方式； 那么，文件头（Inode）就需要包含 13 个指针： 10 个指向数据块的指针； 第 11 个指向索引块的指针； 第 12 个指向二级索引块的指针； 第 13 个指向三级索引块的指针； 所以，这种方式能很灵活地支持小文件和大文件的存放： 对于小文件使用直接查找的方式可减少索引数据块的开销； 对于大文件则以多级索引的方式来支持，所以大文件在访问数据块时需要大量查询； 目录存储基于 Linux 一切皆文件的设计思想，目录其实也是个文件，你甚至可以通过 vim 打开它，它也有 inode，inode 里面也是指向一些块。 和普通文件不同的是，普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。 在目录文件的块中，最简单的保存格式就是列表，就是一项一项地将目录下的文件信息（如文件名、文件 inode、文件类型等）列在表里。 列表中每一项就代表该目录下的文件的文件名和对应的 inode，通过这个 inode，就可以找到真正的文件。 通常，第一项是「.」，表示当前目录，第二项是「..」，表示上一级目录，接下来就是一项一项的文件名和 inode。 如果一个目录有超级多的文件，我们要想在这个目录下找文件，按照列表一项一项的找，效率就不高了。 于是，保存目录的格式改成哈希表，对文件名进行哈希计算，把哈希值保存起来，如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。 目录查询是通过在磁盘上反复搜索完成，需要不断地进行 I/O 操作，开销较大。所以，为了减少 I/O 操作，把当前使用的文件目录缓存在内存，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数，提高了文件系统的访问速度。 空闲空间管理针对磁盘的空闲空间也是要引入管理的机制，接下来介绍几种常见的方法： 空闲表法 空闲链表法 位图法 空闲表法空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图： 当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。 这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。 空闲链表法我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图： 当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。 这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。 空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。 位图法位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。 当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下： 在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。 Linux 文件系统结构用户在创建一个新文件时，Linux 内核会通过 inode 的位图找到空闲可用的 inode，并进行分配。要存储数据时，会通过块的位图找到空闲的块，并分配，但仔细计算一下还是有问题的。 数据块的位图是放在磁盘块里的，假设是放在一个块里，一个块 4K，每位表示一个数据块，共可以表示 4 * 1024 * 8 = 2^15 个空闲块，由于 1 个数据块是 4K 大小，那么最大可以表示的空间为 2^15 * 4 * 1024 = 2^27 个 byte，也就是 128M。 也就是说按照上面的结构，如果采用「一个块的位图 + 一系列的块」，外加「一个块的 inode 的位图 + 一系列的 inode 的结构」能表示的最大空间也就 128M，这太少了，现在很多文件都比这个大。 在 Linux 文件系统，把这个结构称为一个块组，那么有 N 多的块组，就能够表示 N 大的文件。 下图给出了 Linux Ext2 整个文件系统的结构和块组的内容，文件系统都由大量块组组成，在硬盘上相继排布： 最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下： 超级块，包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。 块组描述符，包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。 数据位图和 inode 位图， 用于表示对应的数据块或 inode 是空闲的，还是被使用中。 inode 列表，包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。 数据块，包含文件的有用数据。 你可以会发现每个块组里有很多重复的信息，比如超级块和块组描述符表，这两个都是全局信息，而且非常的重要，这么做是有两个原因： 如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。 通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。 软链接和硬链接有时候我们希望给某个文件取个别名，那么在 Linux 中可以通过硬链接（Hard Link） 和软链接（Symbolic Link） 的方式来实现，它们都是比较特殊的文件，但是实现方式也是不相同的。 硬链接是多个目录项中的「索引节点」指向一个文件，也就是指向同一个 inode，但是 inode 是不可能跨越文件系统的，每个文件系统都有各自的 inode 数据结构和列表，所以硬链接是不可用于跨文件系统的。由于多个目录项都是指向一个 inode，那么只有删除文件的所有硬链接以及源文件时，系统才会彻底删除该文件。 软链接相当于重新创建一个文件，这个文件有独立的 inode，但是这个文件的内容是另外一个文件的路径，所以访问软链接的时候，实际上相当于访问到了另外一个文件，所以软链接是可以跨文件系统的，甚至目标文件被删除了，链接文件还是在的，只不过指向的文件找不到了而已。 日志结构文件系统（Log-structured File System, LFS）促使设计 LFS 的主要原因内存容量变大，磁盘高速缓存也在迅速增加，对文件的访问请求就有可能直接从缓存中读取，而不需要访问磁盘。可以推断，未来多数的磁盘访问都是写操作。 而写操作往往都是零碎的。一个 50us 的磁盘写操作之前常常需要 10ms 的寻道时间以及 4ms 的旋转时间，这样的写操作时效率低下的。 LFS 的设计思想将整个磁盘结构化为一个日志。每个一段时间或是有特殊需要，就将缓存在内存中的所有未决的写操作写入到一个单独的段中，作为在日志末尾的一个邻接段写入磁盘。每一个段的开始都是该段的摘要，内容可能会包括 inode、目录块、数据块等。 这样一来，inode 会散布在整个日志之中，在磁盘中寻找一个 inode 就变得比较困难。因此维护一个由 inode 节点编号索引组成的 inode map。这个map保存在磁盘上，同时也保存在高速内存中。 同样，磁盘的空间不是无限大的，如果不清理不需要的日志，最终整个磁盘都会被占用。因此 LFS 有一个清理线程，周期地扫描日志进行磁盘压缩。 该线程读取日志中各个段的摘要，检查有哪些 inode 和文件。接着根据 inode map 判断当前 inode 节点是否有效以及文件块是否依旧在使用。如果没有使用，则该信息被丢弃；如果仍然在使用，则 inode 和 文件块就进入内存中，等待下一次回写，原来的段被标记为空闲，以便新的日志可以使用。 日志文件系统基于日志结构的文件系统内在的面对出错的鲁棒性被其他的文件系统所借鉴。如 NTFS、 ext3、ReiserFS。 其基本思想是：保存一个用于记录系统下一步要做什么的日志。当系统在即将完成它们的任务前崩溃时，可以通过查看日志的方式，获取出错前的日志，重新执行。 场景分析考虑一个简单的操作：移除文件。在 UNIX 中完成这个操作需要三个步骤： 在目录中删除文件； 释放 inode 到空闲 inode 池中； 将磁盘块归还到空闲磁盘块池中； 如果在步骤一后崩溃，inode 不会再被分配，减少了可使用 inode 的资源； 如果在步骤二后崩溃，磁盘块的丢失，可用的磁盘空间减少。 如果修改操作步骤，先释放 inode，然后发生崩溃，这样 inode 可以被重新分配，但是旧的目录将继续指向这个 inode，因此指向错误文件。 如果先释放磁盘块，后发生崩溃，归还的磁盘块会被重新利用，这样会导致两个甚至更多的 inode 指向同一个磁盘块，但这并不是分享文件的本意。 日志文件系统则在执行这些操作前，先写入一个日志项，记录将要完成的动作，将日志项写入磁盘。只有当日志项写入后，后续操作才能进行。操作完成后，擦除日志项。 被写入日志的操作必须时幂等的，意味着只要有必要，它们可以被重复执行而不会带来破坏。 磁盘配额为防止某一个用户占用太多磁盘空间，多用户操作系统常常提供一种强制性的磁盘配额机制。 其基本思想是系统管理员分配给每个用户拥有的文件和块的最大数量，操作系统确保每个用户不会超过所分配的配额。 每次添加文件块时，文件所有者所用的数据块也会增加，引发对配额硬限制和软限制检查。可以超出软限制，硬限制不可以超出。 硬限制是指对资源节点和数据块的绝对限制,在任何情况下都不允许用户超过这个限制; 而软限制是指用户可以在一定时间范围内(默认时为一周,在/ usr/include/sys/fs/ufs_quota.h 文件中设置)超过软限制的额度,在硬限制的范围内继续申请资源,同时系统会在用户登录时给出警告信息和仍可继续申请资源剩余时间.如果达到时间期限,用户的硬盘使用仍超过软限制的额度,则系统将不允许用户再申请硬盘资源. 文件系统备份备份主要是处理两个潜在的问题： 从灾难中恢复； 从错误的操作中恢复； 对于错误的操作，Windows 的设计者设计了一个特殊的目录——回收站，在删除文件时，文件并不是真正的从磁盘上消失，而是放在了这个特殊目录下，以便需要时还原。 另外，对文件的备份是全部备份还是只备份其中一部分？对于一些能从新获取的文件、临时文件、特殊文件，没有备份的必要。因此合理的方式是只备份特定目录下的文件，而不是整个文件系统。 对前一次备份后没有修改的文件再次备份是一种负担，因而产生了增量转储的思想。简单来说，只对上一次转储后发生变更的数据做备份。 备份的数据往往较多，在写入磁盘前对其压缩就有必要。但是许多压缩算法，磁盘上的单点问题就能导致所有数据无法解压缩。因此压缩与否需要慎重考虑。 虚拟文件系统文件系统的种类众多，而操作系统希望对用户提供一个统一的接口（POSIX 接口），于是在用户层与文件系统层引入了中间层，这个中间层就称为虚拟文件系统（Virtual File System，VFS）。 其关键思想是抽象出所有文件系统都共有的部分，并且将这部分代码单独放在一层，该层调用底层的实际文件系统来管理数据。 大多数 VFS 应用本质上是面向对象的。 当一个文件系统注册时，它做的最基本的工作就是提供一个VFS所需要的函数地址的列表。 如果一个文件系统装载在 /usr 并且一个进程调用它： open(&quot;/usr/include/unistd.h&quot;, O_RDONLY) 当解析路径时，VFS 通过目录项搜索文件对应的 inode，通过 inode 获取对应文件系统的操作函数，创建一个 vnode 并存储这些信息，保存在文件描述符中。 文件 I/O文件的读写方式各有千秋，对于文件的 I/O 分类也非常多，常见的有 缓冲与非缓冲 I/O 直接与非直接 I/O 阻塞与非阻塞 I/O VS 同步与异步 I/O 缓冲与非缓冲 I/O文件操作的标准库是可以实现数据的缓存，那么根据「是否利用标准库缓冲」，可以把文件 I/O 分为缓冲 I/O 和非缓冲 I/O： 缓冲 I/O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件。 非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存。 这里所说的「缓冲」特指标准库内部实现的缓冲。 比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，减少系统调用的次数，毕竟系统调用是有 CPU 上下文切换的开销的。 直接与非直接 I/O我们都知道磁盘 I/O 是非常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调用后，会把用户数据拷贝到内核中缓存起来，这个内核缓存空间也就是「页缓存」，只有当缓存满足某些条件的时候，才发起磁盘 I/O 的请求。 那么，根据是「否利用操作系统的缓存」，可以把文件 I/O 分为直接 I/O 与非直接 I/O： 直接 I/O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。 非直接 I/O，读操作时，数据从内核缓存中拷贝给用户程序，写操作时，数据从用户程序拷贝给内核缓存，再由内核决定什么时候写入数据到磁盘。 如果你在使用文件操作类的系统调用函数时，指定了 O_DIRECT 标志，则表示使用直接 I/O。如果没有设置过，默认使用的是非直接 I/O。 如果用了非直接 I/O 进行写数据操作，内核什么情况下才会把缓存数据写入到磁盘？ 以下几种场景会触发内核缓存的数据写入磁盘： 在调用 write 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上； 用户主动调用 sync，内核缓存会刷到磁盘上； 当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上； 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上； 阻塞与非阻塞 I/O VS 同步与异步 I/O先来看看阻塞 I/O，当用户程序执行 read ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，read 才会返回。 注意，阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程。过程如下图： 知道了阻塞 I/O ，来看看非阻塞 I/O，非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，read 调用才可以获取到结果。过程如下图： 注意，这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。 举个例子，访问管道或 socket 时，如果设置了 O_NONBLOCK 标志，那么就表示使用的是非阻塞 I/O 的方式访问，而不做任何设置的话，默认是阻塞 I/O。 应用程序每次轮询内核的 I/O 是否准备好，感觉有点傻乎乎，因为轮询的过程中，应用程序啥也做不了，只是在循环。 为了解决这种傻乎乎轮询方式，于是 I/O 多路复用技术就出来了，如 select、poll，它是通过 I/O 事件分发，当内核数据准备好时，再以事件通知应用程序进行操作。 这个做法大大改善了应用进程对 CPU 的利用率，在没有被通知的情况下，应用进程可以使用 CPU 做其他的事情。 下图是使用 select I/O 多路复用过程。注意，read 获取数据的过程（数据从内核态拷贝到用户态的过程），也是一个同步的过程，需要等待： 实际上，无论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用都是同步调用。因为它们在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。 而真正的异步 I/O 是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待。 当我们发起 aio_read 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。过程如下图： 下面这张图，总结了以上几种 I/O 模型： 在前面我们知道了，I/O 是分为两个过程的： 数据准备的过程 数据从内核空间拷贝到用户进程缓冲区的过程 阻塞 I/O 会阻塞在「过程 1 」和「过程 2」，而非阻塞 I/O 和基于非阻塞 I/O 的多路复用只会阻塞在「过程 2」，所以这三个都可以认为是同步 I/O。 异步 I/O 则不同，「过程 1 」和「过程 2 」都不会阻塞。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://hbinnn.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"文件系统","slug":"文件系统","permalink":"http://hbinnn.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"}]}],"categories":[{"name":"编译原理","slug":"编译原理","permalink":"http://hbinnn.github.io/categories/%E7%BC%96%E8%AF%91%E5%8E%9F%E7%90%86/"},{"name":"数据库","slug":"数据库","permalink":"http://hbinnn.github.io/categories/%E6%95%B0%E6%8D%AE%E5%BA%93/"},{"name":"网络","slug":"网络","permalink":"http://hbinnn.github.io/categories/%E7%BD%91%E7%BB%9C/"},{"name":"操作系统","slug":"操作系统","permalink":"http://hbinnn.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"读书笔记","slug":"读书笔记","permalink":"http://hbinnn.github.io/tags/%E8%AF%BB%E4%B9%A6%E7%AC%94%E8%AE%B0/"},{"name":"MySQL","slug":"MySQL","permalink":"http://hbinnn.github.io/tags/MySQL/"},{"name":"大杂烩","slug":"大杂烩","permalink":"http://hbinnn.github.io/tags/%E5%A4%A7%E6%9D%82%E7%83%A9/"},{"name":"Linux","slug":"Linux","permalink":"http://hbinnn.github.io/tags/Linux/"},{"name":"端口转发","slug":"端口转发","permalink":"http://hbinnn.github.io/tags/%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/"},{"name":"文件系统","slug":"文件系统","permalink":"http://hbinnn.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"}]}