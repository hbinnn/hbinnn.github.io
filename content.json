{"meta":{"title":"hbinnn's Blog","subtitle":null,"description":null,"author":"hbinnn","url":"http://hbinnn.github.io","root":"/"},"pages":[{"title":"分类","date":"2022-01-23T05:48:07.000Z","updated":"2022-01-23T06:58:35.944Z","comments":true,"path":"categories/index.html","permalink":"http://hbinnn.github.io/categories/index.html","excerpt":"","text":""},{"title":"标签","date":"2022-01-23T05:56:40.000Z","updated":"2022-01-23T06:58:44.236Z","comments":true,"path":"tags/index.html","permalink":"http://hbinnn.github.io/tags/index.html","excerpt":"","text":""}],"posts":[{"title":"Windows与Linux下的端口转发","slug":"Windows与Linux下的端口转发","date":"2022-01-23T15:45:54.000Z","updated":"2022-01-23T15:50:49.073Z","comments":true,"path":"2022/01/23/Windows与Linux下的端口转发/","link":"","permalink":"http://hbinnn.github.io/2022/01/23/Windows%E4%B8%8ELinux%E4%B8%8B%E7%9A%84%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/","excerpt":"","text":"主要涉及的命令有 netsh 与 iptables。 netsh曾经遇到过这样一个场景： PC 可以与 linux1、linux2 互通，但是 linux1 与 linux2 之间不通，linux1 直接访问 linux2 无法成功，也无法通过添加路由的方式访问。此时可以利用 PC 配置端口转发达到访问的目的。 利用 Windows 下的 netsh 命令来配置网络端口转发的功能。 netsh interface portproxy add v4tov4 listenaddress=address listenport=port connectaddress=destaddress connectport=destport protocol=tcp listenaddress：监听地址； listenport：监听端口； connectaddress：端口转发需要连接的目的地址； connectport：端口转发需要连接的目的端口； 配置端口转发之后，连接到 listenaddress:listenport 的 tcp 流量就会被转发到 connectaddress:connectport 中。 查看系统中所有转发规则是否生效：netsh interface portproxy show all； 查看端口转发的设置：netsh interface portproxy dump； 删掉一个特定的端口转发规则：netsh interface portproxy delete v4tov4； 清空当前所有的配置规则：netsh interface portproxy reset； iptablesiptables 是 Linux 下的一个包过滤防火墙管理工具，通过设置过滤规则实现对数据包进出设备以及转发的控制。 iptables 维护着 4 个表和 5 个链，所有的防火墙策略规则都会被写入这些表和链中。 4 个表主要指的是： filter 表：控制数据包是否允许放行，可以控制的链路有 INPUT、FORWARD 和 OUTPUT。 nat 表：控制数据包中地址转换，可以控制的链路有 PREROUTING、INPUT、OUTPUT 和 POSTROUTING。属于同一个流的包只会经过这个表一次。 mangle：修改数据包中的原数据，可以控制的链路有 PREROUTING、INPUT、OUTPUT、FORWARD 和 POSTROUTING。主要用于修改数据包的TOS(Type Of Service，服务类型)、TTL(Time To Live，生存周期)指以及为数据包设置Mark标记，以实现Qos(Quality Of Service，服务质量)调整以及策略路由等应用。 raw：控制数据包是否被状态跟踪机制处理，可以控制的链路有 PREROUTING、OUTPUT。 5个链主要指的是： INPUT：进来的数据包应用此规则链中的策略 OUTPUT：外出的数据包应用此规则链中的策略 FORWARD：转发数据包时应用此规则链中的策略 PREROUTING：对进来的数据包作路由选择前应用此链中的规则 POSTROUTING：对外出的数据包作路由选择后应用此链中的规则 数据报文必须按顺序匹配这些链上的规则。每条链上的各个表的匹配顺序为：raw→mangle→nat→filter。 数据包通过防火墙的流程可以总结为： iptables的命令主要格式如下： iptables [-t 表名] command 匹配规则 常用命令选项： -A 添加防火墙规则 -D 删除防火墙规则 -I 插入防火墙规则 -F 清空防火墙规则 -L 列出添加防火墙规则 -R 替换防火墙规则 -Z 清空防火墙数据表统计信息 -P 设置链默认规则 常用匹配参数： [!]-p 匹配协议，! 表示取反 [!]-s 匹配源地址 [!]-d 匹配目标地址 [!]-i 匹配入站网卡接口 [!]-o 匹配出站网卡接口 [!]–sport 匹配源端口 [!]–dport 匹配目标端口 [!]–src-range 匹配源地址范围 [!]–dst-range 匹配目标地址范围 [!]–limit 四配数据表速率 [!]–mac-source 匹配源MAC地址 [!]–sports 匹配源端口 [!]–dports 匹配目标端口 [!]–stste 匹配状态（INVALID、ESTABLISHED、NEW、RELATED) [!]–string 匹配应用层字串 匹配规则触发动作： ACCEPT 允许数据包通过 DROP 丢弃数据包 REJECT 拒绝数据包通过 LOG 将数据包信息记录 syslog 曰志 DNAT 目标地址转换 SNAT 源地址转换 MASQUERADE 地址欺骗 REDIRECT 重定向 参考资料 Windows端口转发(Port Forwarding in Windows) - 简书 (jianshu.com) iptables详解 | Bruce’s Blog (xiebruce.top) iptables详解及一些常用规则 - 简书 (jianshu.com) Linux iptables命令详解【图文】_wx607823dfcf6a9_51CTO博客","categories":[{"name":"网络","slug":"网络","permalink":"http://hbinnn.github.io/categories/%E7%BD%91%E7%BB%9C/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://hbinnn.github.io/tags/Linux/"},{"name":"端口转发","slug":"端口转发","permalink":"http://hbinnn.github.io/tags/%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/"}]},{"title":"文件系统","slug":"文件系统","date":"2022-01-23T07:21:18.000Z","updated":"2022-01-23T15:46:55.144Z","comments":true,"path":"2022/01/23/文件系统/","link":"","permalink":"http://hbinnn.github.io/2022/01/23/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/","excerpt":"","text":"文章主要摘自博客以及现代操作系统第四章，顺序可能略有调整。 博客原文链接:https://mp.weixin.qq.com/s/qJdoXTv_XS_4ts9YuzMNIw 作者:小林coding 为什么要有文件系统 进程的存储容量受到虚拟地址空间大小的限制； 有些信息需要持久保留； 有些信息需要在多个进程之间共享； 文件系统是什么文件系统是操作系统中负责管理持久数据的子系统。 文件系统的基本数据单位是文件，它的目的是对磁盘上的文件进行组织管理，组织的方式不同，就会形成不同的文件系统。 文件系统类型Linux 支持的文件系统也不少，根据存储位置的不同，可以把文件系统分为三类： 磁盘的文件系统，它是直接把数据存储在磁盘中，比如 Ext 2/3/4、XFS 等都是这类文件系统。 内存的文件系统，这类文件系统的数据不是存储在硬盘的，而是占用内存空间，我们经常用到的 /proc 和 /sys 文件系统都属于这一类，读写这类文件，实际上是读写内核中相关的数据数据。 网络的文件系统，用来访问其他计算机主机数据的文件系统，比如 NFS、SMB 等等 文件文件是进程创建的信息逻辑单元。 文件命名有些文件系统不区分文件名大小写字母，如 MS-DOS，有些则区分，如 UNIX。 有些文件系统文件扩展名只是一种约定，并不强迫使用它，如 UNIX，有些则对扩展名赋予了含义，用户可以在操作系统中注册扩展名，并规定哪个程序“拥有”该扩展名，如 Windows。 文件结构对常见的 UNIX 和 Windows 而言，操作系统不关系文件的内容，操作系统见到的就是字节，文件内容的任何含义只在用户程序中解释。 文件类型 普通文件和类型； 字符特殊文件，和输入/输出有关，用于串行 I/O 类设备； 块特殊文件，用于磁盘类设备； 文件访问 顺序访问，从头开始读取文件的全部字节，不能跳过某一些内容，也无法不按顺序读取； 随机访问文件，以任意次序读取文件的字节； 文件属性 文件使用 我们打开了一个文件后，操作系统会跟踪进程打开的所有文件，所谓的跟踪，就是操作系统为每个进程维护一个打开文件表，文件表里的每一项代表「文件描述符」。 操作系统在打开文件表中维护着打开文件的状态和信息： 文件指针：系统跟踪上次读写位置作为当前文件位置指针，这种指针对打开文件的某个进程来说是唯一的； 文件打开计数器：文件关闭时，操作系统必须重用其打开文件表条目，否则表内空间不够用。因为多个进程可能打开同一个文件，所以系统在删除打开文件条目之前，必须等待最后一个进程关闭文件，该计数器跟踪打开和关闭的数量，当该计数为 0 时，系统关闭文件，删除该条目； 文件磁盘位置：绝大多数文件操作都要求系统修改文件数据，该信息保存在内存中，以免每个操作都从磁盘中读取； 访问权限：每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等），该信息保存在进程的打开文件表中，以便操作系统能允许或拒绝之后的 I/O 请求； 用户和操作系统对文件的读写操作是有差异的，用户习惯以字节的方式读写文件，而操作系统则是以数据块来读写文件，那屏蔽掉这种差异的工作就是文件系统了。 我们来分别看一下，读文件和写文件的过程： 当用户进程从文件读取 1 个字节大小的数据时，文件系统则需要获取字节所在的数据块，再返回数据块对应的用户进程所需的数据部分。 当用户进程把 1 个字节大小的数据写进文件时，文件系统则找到需要写入数据的数据块的位置，然后修改数据块中对应的部分，最后再把数据块写回磁盘。 所以说，文件系统的基本操作单位是数据块。 文件系统实现文件系统布局文件系统存放在磁盘上，磁盘通常被划分为一个或多个分区。 磁盘的 0 号扇区成为主引导记录（Master Boot Record，MBR），MBR 的结尾是分区表，记录着每个分区的起始和结束地址。 分区表中的一个分区被标记为活动分区，它的第一个块被称为引导块，引导块中的程序将装载该分区中的操作系统。 从引导块开始，不同的文件系统，磁盘分区的布局也不一样的。 文件系统如何读写磁盘磁盘读写的最小单位是扇区，扇区的大小只有 512B 大小。 对文件系统而言，如果每次读写都以扇区为单位，那读写的效率会非常低。 所以，文件系统把多个扇区组成了一个逻辑块，每次读写的最小单位就是逻辑块（数据块），Linux 中的逻辑块大小为 4KB，也就是一次性读写 8 个扇区，这将大大提高了磁盘的读写的效率。 索引节点是存储在硬盘上的数据，那么为了加速文件的访问，通常会把索引节点加载到内存中。 超级块，用来存储文件系统的详细信息，比如块个数、块大小、空闲块等等。 索引节点区，用来存储索引节点； 数据块区，用来存储文件或目录数据； 我们不可能把超级块和索引节点区全部加载到内存，这样内存肯定撑不住，所以只有当需要使用的时候，才将其加载进内存，它们加载进内存的时机是不同的： 超级块：当文件系统挂载时进入内存； 索引节点区：当文件被访问时进入内存； Linux 文件系统会为每个文件分配两个数据结构：索引节点（index node）和目录项（directory entry），它们主要用来记录文件的元信息和目录层次结构。 索引节点，也就是 inode（上图中的i节点）用来记录文件的元信息，比如 inode 编号、文件大小、访问权限、创建时间、修改时间、数据在磁盘的位置等等。索引节点是文件的唯一标识，它们之间一一对应，也同样都会被存储在硬盘中，所以索引节点同样占用磁盘空间。 目录项，也就是 dentry，用来记录文件的名字、索引节点指针以及与其他目录项的层级关联关系。多个目录项关联起来，就会形成目录结构，但它与索引节点不同的是，目录项是由内核维护的一个数据结构，不存放于磁盘，而是缓存在内存。 目录项和索引节点的关系是多对一。 目录也是文件，也是用索引节点唯一标识，和普通文件不同的是，普通文件在磁盘里面保存的是文件数据，而目录文件在磁盘里面保存子目录或文件。 文件存储文件的数据是要存储在硬盘上面的，数据在磁盘上的存放方式，就像程序在内存中存放的方式那样，有以下两种： 连续空间存放方式 非连续空间存放方式 其中，非连续空间存放方式又可以分为「链表方式」和「索引方式」。 连续空间存放方式连续空间存放方式顾名思义，文件存放在磁盘「连续的」物理空间中。 这种分配方式有两大优势： 文件的数据都是紧密相连，读写效率很高，因为一次磁盘寻道就可以读出整个文件； 实现简单，只需要记录每个文件的起始磁盘块地址和文件的磁盘块数，就可以确定一个文件； 所以，文件头里需要指定「起始块的位置」和「长度」，有了这两个信息就可以很好的表示文件存放方式是一块连续的磁盘空间。 注意，此处说的文件头，就类似于 Linux 的 inode。 连续空间存放的方式虽然读写效率高，但是有「磁盘空间碎片」和「文件长度不易扩展」的缺陷。 必须先知道一个文件的大小，这样文件系统才会根据文件的大小在磁盘上找到一块连续的空间分配给文件。 对一些只读文件系统，连续空间存放的方案反而比较高效。CD-ROM、DVD 等。 非连续空间存放方式非连续空间存放方式分为「链表方式」和「索引方式」。 链表方式链表的方式存放是离散的，不用连续的，于是就可以消除磁盘碎片，可大大提高磁盘空间的利用率，同时文件的长度可以动态扩展。根据实现的方式的不同，链表可分为「隐式链表」和「显式链接」两种形式。 隐式链表文件要以「隐式链表」的方式存放的话，实现的方式是文件头要包含「第一块」和「最后一块」的位置，并且每个数据块里面留出一个指针空间，用来存放下一个数据块的位置，这样一个数据块连着一个数据块，从链头开是就可以顺着指针找到所有的数据块，所以存放的方式可以是不连续的。 隐式链表的存放方式的缺点在于无法直接访问数据块，只能通过指针顺序访问文件，以及数据块指针消耗了一定的存储空间。 另一方面，由于指针占用了一些字节，每个磁盘块中存储数据的字节数不再是2的整数次幂，对于以长度为2的整数次幂来读写磁盘块的程序而言，降低了运行效率。 显式链接如果取出每个磁盘块的指针，把它放在内存的一个表中，就可以解决上述隐式链表的两个不足。那么，这种实现方式是「显式链接」，它指把用于链接文件各数据块的指针，显式地存放在内存的一张链接表中，该表在整个磁盘仅设置一张，每个表项中存放链接指针，指向下一个数据块号。 对于显式链接的工作方式，我们举个例子，文件 A 依次使用了磁盘块 4、7、2、10 和 12 ，文件 B 依次使用了磁盘块 6、3、11 和 14 。利用下图中的表，可以从第 4 块开始，顺着链走到最后，找到文件 A 的全部磁盘块。同样，从第 6 块开始，顺着链走到最后，也能够找出文件 B 的全部磁盘块。最后，这两个链都以一个不属于有效磁盘编号的特殊标记（如 -1 ）结束。内存中的这样一个表格称为文件分配表（File Allocation Table，FAT）。 由于查找记录的过程是在内存中进行的，因而不仅显著地提高了检索速度，而且大大减少了访问磁盘的次数。但也正是整个表都存放在内存中的关系，它的主要的缺点是不适用于大磁盘。 比如，对于 200GB 的磁盘和 1KB 大小的块，这张表需要有 2 亿项，每一项对应于这 2 亿个磁盘块中的一个块，每项如果需要 4 个字节，那这张表要占用 800MB 内存，很显然 FAT 方案对于大磁盘而言不太合适。 索引方式链表的方式解决了连续分配的磁盘碎片和文件动态扩展的问题，但是不能有效支持直接访问（FAT除外），索引的方式可以解决这个问题。 索引的实现是为每个文件创建一个「索引数据块」，里面存放的是指向文件数据块的指针列表，说白了就像书的目录一样，要找哪个章节的内容，看目录查就可以。 另外，文件头需要包含指向「索引数据块」的指针，这样就可以通过文件头知道索引数据块的位置，再通过索引数据块里的索引信息找到对应的数据块。 创建文件时，索引块的所有指针都设为空。当首次写入第 i 块时，先从空闲空间中取得一个块，再将其地址写到索引块的第 i 个条目。 索引的方式优点在于： 文件的创建、增大、缩小很方便； 不会有碎片的问题； 支持顺序读写和随机读写； 由于索引数据也是存放在磁盘块的，如果文件很小，明明只需一块就可以存放的下，但还是需要额外分配一块来存放索引数据，所以缺陷之一就是存储索引带来的开销。 如果文件很大，大到一个索引数据块放不下索引信息，这时又要如何处理大文件的存放呢？我们可以通过组合的方式，来处理大文件的存。 先来看看链表 + 索引的组合，这种组合称为「链式索引块」，它的实现方式是在索引数据块留出一个存放下一个索引数据块的指针，于是当一个索引数据块的索引信息用完了，就可以通过指针的方式，找到下一个索引数据块的信息。 这种方式也会出现前面提到的链表方式的问题。 还有另外一种组合方式是索引 + 索引的方式，这种组合称为「多级索引块」，实现方式是通过一个索引块来存放多个索引数据块，一层套一层索引。 Unix 文件存储方式我们先把前面提到的文件存储方式，做个比较： 那早期 Unix 文件系统是组合了前面的文件存放方式的优点，如下图： 它是根据文件的大小，存放的方式会有所变化： 如果存放文件所需的数据块小于 10 块，则采用直接查找的方式； 如果存放文件所需的数据块超过 10 块，则采用一级间接索引方式； 如果前面两种方式都不够存放大文件，则采用二级间接索引方式； 如果二级间接索引也不够存放大文件，这采用三级间接索引方式； 那么，文件头（Inode）就需要包含 13 个指针： 10 个指向数据块的指针； 第 11 个指向索引块的指针； 第 12 个指向二级索引块的指针； 第 13 个指向三级索引块的指针； 所以，这种方式能很灵活地支持小文件和大文件的存放： 对于小文件使用直接查找的方式可减少索引数据块的开销； 对于大文件则以多级索引的方式来支持，所以大文件在访问数据块时需要大量查询； 目录存储基于 Linux 一切皆文件的设计思想，目录其实也是个文件，你甚至可以通过 vim 打开它，它也有 inode，inode 里面也是指向一些块。 和普通文件不同的是，普通文件的块里面保存的是文件数据，而目录文件的块里面保存的是目录里面一项一项的文件信息。 在目录文件的块中，最简单的保存格式就是列表，就是一项一项地将目录下的文件信息（如文件名、文件 inode、文件类型等）列在表里。 列表中每一项就代表该目录下的文件的文件名和对应的 inode，通过这个 inode，就可以找到真正的文件。 通常，第一项是「.」，表示当前目录，第二项是「..」，表示上一级目录，接下来就是一项一项的文件名和 inode。 如果一个目录有超级多的文件，我们要想在这个目录下找文件，按照列表一项一项的找，效率就不高了。 于是，保存目录的格式改成哈希表，对文件名进行哈希计算，把哈希值保存起来，如果我们要查找一个目录下面的文件名，可以通过名称取哈希。如果哈希能够匹配上，就说明这个文件的信息在相应的块里面。 目录查询是通过在磁盘上反复搜索完成，需要不断地进行 I/O 操作，开销较大。所以，为了减少 I/O 操作，把当前使用的文件目录缓存在内存，以后要使用该文件时只要在内存中操作，从而降低了磁盘操作次数，提高了文件系统的访问速度。 空闲空间管理针对磁盘的空闲空间也是要引入管理的机制，接下来介绍几种常见的方法： 空闲表法 空闲链表法 位图法 空闲表法空闲表法就是为所有空闲空间建立一张表，表内容包括空闲区的第一个块号和该空闲区的块个数，注意，这个方式是连续分配的。如下图： 当请求分配磁盘空间时，系统依次扫描空闲表里的内容，直到找到一个合适的空闲区域为止。当用户撤销一个文件时，系统回收文件空间。这时，也需顺序扫描空闲表，寻找一个空闲表条目并将释放空间的第一个物理块号及它占用的块数填到这个条目中。 这种方法仅当有少量的空闲区时才有较好的效果。因为，如果存储空间中有着大量的小的空闲区，则空闲表变得很大，这样查询效率会很低。另外，这种分配技术适用于建立连续文件。 空闲链表法我们也可以使用「链表」的方式来管理空闲空间，每一个空闲块里有一个指针指向下一个空闲块，这样也能很方便的找到空闲块并管理起来。如下图： 当创建文件需要一块或几块时，就从链头上依次取下一块或几块。反之，当回收空间时，把这些空闲块依次接到链头上。 这种技术只要在主存中保存一个指针，令它指向第一个空闲块。其特点是简单，但不能随机访问，工作效率低，因为每当在链上增加或移动空闲块时需要做很多 I/O 操作，同时数据块的指针消耗了一定的存储空间。 空闲表法和空闲链表法都不适合用于大型文件系统，因为这会使空闲表或空闲链表太大。 位图法位图是利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。 当值为 0 时，表示对应的盘块空闲，值为 1 时，表示对应的盘块已分配。它形式如下： 在 Linux 文件系统就采用了位图的方式来管理空闲空间，不仅用于数据空闲块的管理，还用于 inode 空闲块的管理，因为 inode 也是存储在磁盘的，自然也要有对其管理。 Linux 文件系统结构用户在创建一个新文件时，Linux 内核会通过 inode 的位图找到空闲可用的 inode，并进行分配。要存储数据时，会通过块的位图找到空闲的块，并分配，但仔细计算一下还是有问题的。 数据块的位图是放在磁盘块里的，假设是放在一个块里，一个块 4K，每位表示一个数据块，共可以表示 4 * 1024 * 8 = 2^15 个空闲块，由于 1 个数据块是 4K 大小，那么最大可以表示的空间为 2^15 * 4 * 1024 = 2^27 个 byte，也就是 128M。 也就是说按照上面的结构，如果采用「一个块的位图 + 一系列的块」，外加「一个块的 inode 的位图 + 一系列的 inode 的结构」能表示的最大空间也就 128M，这太少了，现在很多文件都比这个大。 在 Linux 文件系统，把这个结构称为一个块组，那么有 N 多的块组，就能够表示 N 大的文件。 下图给出了 Linux Ext2 整个文件系统的结构和块组的内容，文件系统都由大量块组组成，在硬盘上相继排布： 最前面的第一个块是引导块，在系统启动时用于启用引导，接着后面就是一个一个连续的块组了，块组的内容如下： 超级块，包含的是文件系统的重要信息，比如 inode 总个数、块总个数、每个块组的 inode 个数、每个块组的块个数等等。 块组描述符，包含文件系统中各个块组的状态，比如块组中空闲块和 inode 的数目等，每个块组都包含了文件系统中「所有块组的组描述符信息」。 数据位图和 inode 位图， 用于表示对应的数据块或 inode 是空闲的，还是被使用中。 inode 列表，包含了块组中所有的 inode，inode 用于保存文件系统中与各个文件和目录相关的所有元数据。 数据块，包含文件的有用数据。 你可以会发现每个块组里有很多重复的信息，比如超级块和块组描述符表，这两个都是全局信息，而且非常的重要，这么做是有两个原因： 如果系统崩溃破坏了超级块或块组描述符，有关文件系统结构和内容的所有信息都会丢失。如果有冗余的副本，该信息是可能恢复的。 通过使文件和管理数据尽可能接近，减少了磁头寻道和旋转，这可以提高文件系统的性能。 软链接和硬链接有时候我们希望给某个文件取个别名，那么在 Linux 中可以通过硬链接（Hard Link） 和软链接（Symbolic Link） 的方式来实现，它们都是比较特殊的文件，但是实现方式也是不相同的。 硬链接是多个目录项中的「索引节点」指向一个文件，也就是指向同一个 inode，但是 inode 是不可能跨越文件系统的，每个文件系统都有各自的 inode 数据结构和列表，所以硬链接是不可用于跨文件系统的。由于多个目录项都是指向一个 inode，那么只有删除文件的所有硬链接以及源文件时，系统才会彻底删除该文件。 软链接相当于重新创建一个文件，这个文件有独立的 inode，但是这个文件的内容是另外一个文件的路径，所以访问软链接的时候，实际上相当于访问到了另外一个文件，所以软链接是可以跨文件系统的，甚至目标文件被删除了，链接文件还是在的，只不过指向的文件找不到了而已。 日志结构文件系统（Log-structured File System, LFS）促使设计 LFS 的主要原因内存容量变大，磁盘高速缓存也在迅速增加，对文件的访问请求就有可能直接从缓存中读取，而不需要访问磁盘。可以推断，未来多数的磁盘访问都是写操作。 而写操作往往都是零碎的。一个 50us 的磁盘写操作之前常常需要 10ms 的寻道时间以及 4ms 的旋转时间，这样的写操作时效率低下的。 LFS 的设计思想将整个磁盘结构化为一个日志。每个一段时间或是有特殊需要，就将缓存在内存中的所有未决的写操作写入到一个单独的段中，作为在日志末尾的一个邻接段写入磁盘。每一个段的开始都是该段的摘要，内容可能会包括 inode、目录块、数据块等。 这样一来，inode 会散布在整个日志之中，在磁盘中寻找一个 inode 就变得比较困难。因此维护一个由 inode 节点编号索引组成的 inode map。这个map保存在磁盘上，同时也保存在高速内存中。 同样，磁盘的空间不是无限大的，如果不清理不需要的日志，最终整个磁盘都会被占用。因此 LFS 有一个清理线程，周期地扫描日志进行磁盘压缩。 该线程读取日志中各个段的摘要，检查有哪些 inode 和文件。接着根据 inode map 判断当前 inode 节点是否有效以及文件块是否依旧在使用。如果没有使用，则该信息被丢弃；如果仍然在使用，则 inode 和 文件块就进入内存中，等待下一次回写，原来的段被标记为空闲，以便新的日志可以使用。 日志文件系统基于日志结构的文件系统内在的面对出错的鲁棒性被其他的文件系统所借鉴。如 NTFS、 ext3、ReiserFS。 其基本思想是：保存一个用于记录系统下一步要做什么的日志。当系统在即将完成它们的任务前崩溃时，可以通过查看日志的方式，获取出错前的日志，重新执行。 场景分析考虑一个简单的操作：移除文件。在 UNIX 中完成这个操作需要三个步骤： 在目录中删除文件； 释放 inode 到空闲 inode 池中； 将磁盘块归还到空闲磁盘块池中； 如果在步骤一后崩溃，inode 不会再被分配，减少了可使用 inode 的资源； 如果在步骤二后崩溃，磁盘块的丢失，可用的磁盘空间减少。 如果修改操作步骤，先释放 inode，然后发生崩溃，这样 inode 可以被重新分配，但是旧的目录将继续指向这个 inode，因此指向错误文件。 如果先释放磁盘块，后发生崩溃，归还的磁盘块会被重新利用，这样会导致两个甚至更多的 inode 指向同一个磁盘块，但这并不是分享文件的本意。 日志文件系统则在执行这些操作前，先写入一个日志项，记录将要完成的动作，将日志项写入磁盘。只有当日志项写入后，后续操作才能进行。操作完成后，擦除日志项。 被写入日志的操作必须时幂等的，意味着只要有必要，它们可以被重复执行而不会带来破坏。 磁盘配额为防止某一个用户占用太多磁盘空间，多用户操作系统常常提供一种强制性的磁盘配额机制。 其基本思想是系统管理员分配给每个用户拥有的文件和块的最大数量，操作系统确保每个用户不会超过所分配的配额。 每次添加文件块时，文件所有者所用的数据块也会增加，引发对配额硬限制和软限制检查。可以超出软限制，硬限制不可以超出。 硬限制是指对资源节点和数据块的绝对限制,在任何情况下都不允许用户超过这个限制; 而软限制是指用户可以在一定时间范围内(默认时为一周,在/ usr/include/sys/fs/ufs_quota.h 文件中设置)超过软限制的额度,在硬限制的范围内继续申请资源,同时系统会在用户登录时给出警告信息和仍可继续申请资源剩余时间.如果达到时间期限,用户的硬盘使用仍超过软限制的额度,则系统将不允许用户再申请硬盘资源. 文件系统备份备份主要是处理两个潜在的问题： 从灾难中恢复； 从错误的操作中恢复； 对于错误的操作，Windows 的设计者设计了一个特殊的目录——回收站，在删除文件时，文件并不是真正的从磁盘上消失，而是放在了这个特殊目录下，以便需要时还原。 另外，对文件的备份是全部备份还是只备份其中一部分？对于一些能从新获取的文件、临时文件、特殊文件，没有备份的必要。因此合理的方式是只备份特定目录下的文件，而不是整个文件系统。 对前一次备份后没有修改的文件再次备份是一种负担，因而产生了增量转储的思想。简单来说，只对上一次转储后发生变更的数据做备份。 备份的数据往往较多，在写入磁盘前对其压缩就有必要。但是许多压缩算法，磁盘上的单点问题就能导致所有数据无法解压缩。因此压缩与否需要慎重考虑。 虚拟文件系统文件系统的种类众多，而操作系统希望对用户提供一个统一的接口（POSIX 接口），于是在用户层与文件系统层引入了中间层，这个中间层就称为虚拟文件系统（Virtual File System，VFS）。 其关键思想是抽象出所有文件系统都共有的部分，并且将这部分代码单独放在一层，该层调用底层的实际文件系统来管理数据。 大多数 VFS 应用本质上是面向对象的。 当一个文件系统注册时，它做的最基本的工作就是提供一个VFS所需要的函数地址的列表。 如果一个文件系统装载在 /usr 并且一个进程调用它： open(&quot;/usr/include/unistd.h&quot;, O_RDONLY) 当解析路径时，VFS 通过目录项搜索文件对应的 inode，通过 inode 获取对应文件系统的操作函数，创建一个 vnode 并存储这些信息，保存在文件描述符中。 文件 I/O文件的读写方式各有千秋，对于文件的 I/O 分类也非常多，常见的有 缓冲与非缓冲 I/O 直接与非直接 I/O 阻塞与非阻塞 I/O VS 同步与异步 I/O 缓冲与非缓冲 I/O文件操作的标准库是可以实现数据的缓存，那么根据「是否利用标准库缓冲」，可以把文件 I/O 分为缓冲 I/O 和非缓冲 I/O： 缓冲 I/O，利用的是标准库的缓存实现文件的加速访问，而标准库再通过系统调用访问文件。 非缓冲 I/O，直接通过系统调用访问文件，不经过标准库缓存。 这里所说的「缓冲」特指标准库内部实现的缓冲。 比方说，很多程序遇到换行时才真正输出，而换行前的内容，其实就是被标准库暂时缓存了起来，这样做的目的是，减少系统调用的次数，毕竟系统调用是有 CPU 上下文切换的开销的。 直接与非直接 I/O我们都知道磁盘 I/O 是非常慢的，所以 Linux 内核为了减少磁盘 I/O 次数，在系统调用后，会把用户数据拷贝到内核中缓存起来，这个内核缓存空间也就是「页缓存」，只有当缓存满足某些条件的时候，才发起磁盘 I/O 的请求。 那么，根据是「否利用操作系统的缓存」，可以把文件 I/O 分为直接 I/O 与非直接 I/O： 直接 I/O，不会发生内核缓存和用户程序之间数据复制，而是直接经过文件系统访问磁盘。 非直接 I/O，读操作时，数据从内核缓存中拷贝给用户程序，写操作时，数据从用户程序拷贝给内核缓存，再由内核决定什么时候写入数据到磁盘。 如果你在使用文件操作类的系统调用函数时，指定了 O_DIRECT 标志，则表示使用直接 I/O。如果没有设置过，默认使用的是非直接 I/O。 如果用了非直接 I/O 进行写数据操作，内核什么情况下才会把缓存数据写入到磁盘？ 以下几种场景会触发内核缓存的数据写入磁盘： 在调用 write 的最后，当发现内核缓存的数据太多的时候，内核会把数据写到磁盘上； 用户主动调用 sync，内核缓存会刷到磁盘上； 当内存十分紧张，无法再分配页面时，也会把内核缓存的数据刷到磁盘上； 内核缓存的数据的缓存时间超过某个时间时，也会把数据刷到磁盘上； 阻塞与非阻塞 I/O VS 同步与异步 I/O先来看看阻塞 I/O，当用户程序执行 read ，线程会被阻塞，一直等到内核数据准备好，并把数据从内核缓冲区拷贝到应用程序的缓冲区中，当拷贝过程完成，read 才会返回。 注意，阻塞等待的是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程。过程如下图： 知道了阻塞 I/O ，来看看非阻塞 I/O，非阻塞的 read 请求在数据未准备好的情况下立即返回，可以继续往下执行，此时应用程序不断轮询内核，直到数据准备好，内核将数据拷贝到应用程序缓冲区，read 调用才可以获取到结果。过程如下图： 注意，这里最后一次 read 调用，获取数据的过程，是一个同步的过程，是需要等待的过程。这里的同步指的是内核态的数据拷贝到用户程序的缓存区这个过程。 举个例子，访问管道或 socket 时，如果设置了 O_NONBLOCK 标志，那么就表示使用的是非阻塞 I/O 的方式访问，而不做任何设置的话，默认是阻塞 I/O。 应用程序每次轮询内核的 I/O 是否准备好，感觉有点傻乎乎，因为轮询的过程中，应用程序啥也做不了，只是在循环。 为了解决这种傻乎乎轮询方式，于是 I/O 多路复用技术就出来了，如 select、poll，它是通过 I/O 事件分发，当内核数据准备好时，再以事件通知应用程序进行操作。 这个做法大大改善了应用进程对 CPU 的利用率，在没有被通知的情况下，应用进程可以使用 CPU 做其他的事情。 下图是使用 select I/O 多路复用过程。注意，read 获取数据的过程（数据从内核态拷贝到用户态的过程），也是一个同步的过程，需要等待： 实际上，无论是阻塞 I/O、非阻塞 I/O，还是基于非阻塞 I/O 的多路复用都是同步调用。因为它们在 read 调用时，内核将数据从内核空间拷贝到应用程序空间，过程都是需要等待的，也就是说这个过程是同步的，如果内核实现的拷贝效率不高，read 调用就会在这个同步过程中等待比较长的时间。 而真正的异步 I/O 是「内核数据准备好」和「数据从内核态拷贝到用户态」这两个过程都不用等待。 当我们发起 aio_read 之后，就立即返回，内核自动将数据从内核空间拷贝到应用程序空间，这个拷贝过程同样是异步的，内核自动完成的，和前面的同步操作不一样，应用程序并不需要主动发起拷贝动作。过程如下图： 下面这张图，总结了以上几种 I/O 模型： 在前面我们知道了，I/O 是分为两个过程的： 数据准备的过程 数据从内核空间拷贝到用户进程缓冲区的过程 阻塞 I/O 会阻塞在「过程 1 」和「过程 2」，而非阻塞 I/O 和基于非阻塞 I/O 的多路复用只会阻塞在「过程 2」，所以这三个都可以认为是同步 I/O。 异步 I/O 则不同，「过程 1 」和「过程 2 」都不会阻塞。","categories":[{"name":"操作系统","slug":"操作系统","permalink":"http://hbinnn.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"文件系统","slug":"文件系统","permalink":"http://hbinnn.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"}]}],"categories":[{"name":"网络","slug":"网络","permalink":"http://hbinnn.github.io/categories/%E7%BD%91%E7%BB%9C/"},{"name":"操作系统","slug":"操作系统","permalink":"http://hbinnn.github.io/categories/%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F/"}],"tags":[{"name":"Linux","slug":"Linux","permalink":"http://hbinnn.github.io/tags/Linux/"},{"name":"端口转发","slug":"端口转发","permalink":"http://hbinnn.github.io/tags/%E7%AB%AF%E5%8F%A3%E8%BD%AC%E5%8F%91/"},{"name":"文件系统","slug":"文件系统","permalink":"http://hbinnn.github.io/tags/%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/"}]}